{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first project requires you to implement a scanner, and a parser for the uC language, specified by [uC BNF Grammar](./uC_Grammar.ipynb) notebook. Study the specification of uC grammar carefully. To complete this first project, you will use the [PLY](http://www.dabeaz.com/ply/), a Python version of the [lex/yacc](http://dinosaur.compilertools.net/) toolset with same functionality but with a friendlier interface. Please read the complete contents of this section and carefully complete the steps indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are concise ways of describing a set of strings that meet a given pattern. For example, we can specify the regular expression:\n",
    "```\n",
    "r'[a-zA-Z_][0-9a-zA-Z_]*'\n",
    "``` \n",
    "to describe valid identifiers in the uC language. Regular expressions are a mini-language that lets you specify the rules for constructing a string set. This specification mini-language is very similar between the different programming languages that contain the concept of regular expressions (also called RE or REGEX). Thus, learning to write regular expressions in Python will also be useful for describing REs in other programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a set of regular expressions that will be used by the lexical parser to recognize the following patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid uC identifiers\n",
    "identifier = r'[a-zA-Z_][0-9a-zA-Z_]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer constants\n",
    "int_const = r'0|[1-9][0-9]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floating constants\n",
    "float_const = r'([0-9]*\\.[0-9]+)|([0-9]+\\.)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments in C-Style /* ... */\n",
    "ccomment = r'/\\*(.|\\n)*?\\*/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unterminated C-style comment\n",
    "uccomment = r'/\\*(.|\\n)*$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C++-style comment (//...)\n",
    "cppcomment = r'//.*\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_literal\n",
    "string_literal = r'\".*?\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmatched_quote\n",
    "unquote = r'\".*$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "import re\n",
    "b = re.match(ccomment, \"/***/\")\n",
    "if b:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministc Finite Automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical analyzer turns your input program into a set of tokens that are the language's accepted keywords, names, and punctuations. At the heart of this transition is the formalism known as finite automata. These are essentially graphs, like transition diagrams, with a few differences:\n",
    "\n",
    "1. Finite automata are recognizers; they simply say \"yes\" or \"no\" about each possible input string.\n",
    "\n",
    "2. Finite automata come in two flavors:\n",
    "\n",
    "(a) Non-deterministic finite automata (NFA) have no restrictions on their edge labels. A symbol can label several edges out of the same state, and $\\in$, the empty string, is a possible label.\n",
    "\n",
    "(b) Deterministic finite automata (DFA) have, for each state and for each symbol of its input alphabet, exactly one edge with that symbol leaving that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An DFA accepts input string x if and only if there is some path in the transition graph from the start state to one of the accepting states, such that the symbols along the path spell out x. The following algorithm shows how to implement a simple DFA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DFA Simple Implementation Example:\n",
    "class DFA:\n",
    "    current_state = None;\n",
    "    def __init__(self, states, alphabet, transition_function, start_state, accept_states):\n",
    "        self.states = states;\n",
    "        self.alphabet = alphabet;\n",
    "        self.transition_function = transition_function;\n",
    "        self.start_state = start_state;\n",
    "        self.accept_states = accept_states;\n",
    "        self.current_state = start_state;\n",
    "        return;\n",
    "    \n",
    "    def transition_to_state_with_input(self, input_value):\n",
    "        if ((self.current_state, input_value) not in self.transition_function.keys()):\n",
    "            self.current_state = None;\n",
    "            return;\n",
    "        self.current_state = self.transition_function[(self.current_state, input_value)];\n",
    "        return;\n",
    "    \n",
    "    def in_accept_state(self):\n",
    "        return self.current_state in accept_states;\n",
    "    \n",
    "    def go_to_initial_state(self):\n",
    "        self.current_state = self.start_state;\n",
    "        return;\n",
    "    \n",
    "    def run_with_input_list(self, input_list):\n",
    "        self.go_to_initial_state();\n",
    "        for inp in input_list:\n",
    "            self.transition_to_state_with_input(inp);\n",
    "            continue;\n",
    "        return self.in_accept_state();\n",
    "    pass;\n",
    "\n",
    "states = {0, 1, 2, 3};\n",
    "alphabet = {'a', 'b', 'c', 'd'};\n",
    "\n",
    "tf = dict();\n",
    "tf[(0, 'a')] = 1;\n",
    "tf[(0, 'b')] = 2;\n",
    "tf[(0, 'c')] = 3;\n",
    "tf[(0, 'd')] = 0;\n",
    "tf[(1, 'a')] = 1;\n",
    "tf[(1, 'b')] = 2;\n",
    "tf[(1, 'c')] = 3;\n",
    "tf[(1, 'd')] = 0;\n",
    "tf[(2, 'a')] = 1;\n",
    "tf[(2, 'b')] = 2;\n",
    "tf[(2, 'c')] = 3;\n",
    "tf[(2, 'd')] = 0;\n",
    "tf[(3, 'a')] = 1;\n",
    "tf[(3, 'b')] = 2;\n",
    "tf[(3, 'c')] = 3;\n",
    "tf[(3, 'd')] = 0;\n",
    "start_state = 0;\n",
    "accept_states = {2, 3};\n",
    "\n",
    "d = DFA(states, alphabet, tf, start_state, accept_states);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_program = list('abcabdc')\n",
    "print(d.run_with_input_list(inp_program));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_program = list('abcdabcdabcd');\n",
    "print(d.run_with_input_list(inp_program));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Change the DFA implementation example above for each of the following languages:\n",
    "\n",
    "(a) All strings over {a, b, c} that contain an odd number of b’s\n",
    "\n",
    "(b) All strings over {a, b, c} that contain an even number of a’s and an odd number of b’s\n",
    "\n",
    "(c) All strings over {a, b, c, d} of length at least 2 whose second symbol does not appear elsewhere in the string. So bdabc, acbab, bacbd, abcdc $\\in$ L, while aa, bcabc, abcbc, dd $\\not\\in$ L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Lex\n",
    "The process of “lexing” is that of taking input text and breaking it down into a stream of tokens. Each token is like a valid word from the dictionary. Essentially, the role of the lexer is to simply make sure that the input text consists of valid symbols and tokens prior to any further processing related to parsing.\n",
    "\n",
    "Each token is defined by a regular expression. Thus, your task here is to define a set of regular expressions for the uC language. The actual job of lexing will be handled by PLY. For a better understanding study the [Lex](http://www.dabeaz.com/ply/ply.html#ply_nn3) chapter in the PLY documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification\n",
    "Your lexer must recognize the symbols and tokens of uC Grammar. For instance, in the example below, the name on the left is the token name, and the value on the right is the matching text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserved Keywords:\n",
    "```\n",
    "    FOR   : ’for’\n",
    "    IF    : ’if’\n",
    "    PRINT : ’print’\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers:\n",
    "```\n",
    "    ID    : any text starting with a letter or ’_’, followed by any number of letters,\n",
    "            digits, or underscores, that is not a reserved word.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Operators and Delimiters:\n",
    "```\n",
    "    PLUS    : '+'\n",
    "    MINUS   : '-'\n",
    "    TIMES   : '*'\n",
    "    DIVIDE  : ’/’\n",
    "    ASSIGN  : ’=’\n",
    "    SEMI    : ’;’\n",
    "    LPAREN  : ’(’\n",
    "    RPAREN  : ’)’\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literals:\n",
    "```\n",
    "    INT_CONST : 123\n",
    "    FLOAT_CONST : 1.234\n",
    "    STRING_LITERAL : \"Hello World\\n\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:  To be ignored by your lexer\n",
    "```\n",
    "     //             Skips the rest of the line\n",
    "     /* ... */      Skips a block (no nesting allowed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors: Your lexer must report the following error messages:\n",
    "```\n",
    "     lineno: Unterminated string\n",
    "     lineno: Unterminated comment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lex Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex\n",
    "\n",
    "\n",
    "class UCLexer():\n",
    "    \"\"\" A lexer for the uC language. After building it, set the\n",
    "        input text with input(), and call token() to get new\n",
    "        tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, error_func):\n",
    "        \"\"\" Create a new Lexer.\n",
    "            An error function. Will be called with an error\n",
    "            message, line and column as arguments, in case of\n",
    "            an error during lexing.\n",
    "        \"\"\"\n",
    "        self.error_func = error_func\n",
    "        self.filename = ''\n",
    "\n",
    "        # Keeps track of the last token returned from self.token()\n",
    "        self.last_token = None\n",
    "\n",
    "    def build(self, **kwargs):\n",
    "        \"\"\" Builds the lexer from the specification. Must be\n",
    "            called after the lexer object is created.\n",
    "\n",
    "            This method exists separately, because the PLY\n",
    "            manual warns against calling lex.lex inside __init__\n",
    "        \"\"\"\n",
    "        self.lexer = lex.lex(object=self, **kwargs)\n",
    "\n",
    "    def reset_lineno(self):\n",
    "        \"\"\" Resets the internal line number counter of the lexer.\n",
    "        \"\"\"\n",
    "        self.lexer.lineno = 1\n",
    "\n",
    "    def input(self, text):\n",
    "        self.lexer.input(text)\n",
    "\n",
    "    def token(self):\n",
    "        self.last_token = self.lexer.token()\n",
    "        return self.last_token\n",
    "\n",
    "    def find_tok_column(self, token):\n",
    "        \"\"\" Find the column of the token in its line.\n",
    "        \"\"\"\n",
    "        last_cr = self.lexer.lexdata.rfind('\\n', 0, token.lexpos)\n",
    "        return token.lexpos - last_cr\n",
    "\n",
    "    # Internal auxiliary methods\n",
    "    def _error(self, msg, token):\n",
    "        location = self._make_tok_location(token)\n",
    "        self.error_func(msg, location[0], location[1])\n",
    "        self.lexer.skip(1)\n",
    "\n",
    "    def _make_tok_location(self, token):\n",
    "        return (token.lineno, self.find_tok_column(token))\n",
    "\n",
    "    # Reserved keywords\n",
    "    keywords = (\n",
    "        'ASSERT', 'BREAK', 'CHAR', 'ELSE', 'FLOAT', 'FOR', 'IF',\n",
    "        'INT', 'PRINT', 'READ', 'RETURN', 'VOID', 'WHILE',\n",
    "    )\n",
    "\n",
    "    keyword_map = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_map[keyword.lower()] = keyword\n",
    "\n",
    "    #\n",
    "    # All the tokens recognized by the lexer\n",
    "    #\n",
    "    tokens = keywords + (\n",
    "        # Identifiers\n",
    "        'ID',\n",
    "\n",
    "        # constants\n",
    "        'INT_CONST', 'FLOAT_CONST',\n",
    "\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Rules\n",
    "    #\n",
    "    t_ignore = ' \\t'\n",
    "\n",
    "    # Newlines\n",
    "    def t_NEWLINE(self, t):\n",
    "        r'\\n+'\n",
    "        t.lexer.lineno += t.value.count(\"\\n\")\n",
    "\n",
    "    def t_ID(self, t):\n",
    "        r'[a-zA-Z_][0-9a-zA-Z_]*'\n",
    "        t.type = self.keyword_map.get(t.value, \"ID\")\n",
    "        return t\n",
    "\n",
    "    def t_comment(self, t):\n",
    "        r'/\\*(.|\\n)*?\\*/'\n",
    "        t.lexer.lineno += t.value.count('\\n')\n",
    "\n",
    "    def t_error(self, t):\n",
    "        msg = \"Illegal character %s\" % repr(t.value[0])\n",
    "        self._error(msg, t)\n",
    "\n",
    "    # Scanner (used only for test)\n",
    "    def scan(self, data):\n",
    "        self.lexer.input(data)\n",
    "        while True:\n",
    "            tok = self.lexer.token()\n",
    "            if not tok:\n",
    "                break\n",
    "            print(tok)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import sys\n",
    "\n",
    "    def print_error(msg, x, y):\n",
    "        print(\"Lexical error: %s at %d:%d\" % (msg, x, y))\n",
    "\n",
    "    m = UCLexer(print_error)\n",
    "    m.build()  # Build the lexer\n",
    "    m.scan(open(sys.argv[1]).read())  # print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "For initial development, try running the lexer on a sample input file such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* comment */\n",
    "int j = 3;\n",
    "int main () {\n",
    "  int i = j;\n",
    "  int k = 3;\n",
    "  int p = 2 * j;\n",
    "  assert p == 2 * i;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result will look similar to the text shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LexToken(INT,'int',2,14)\n",
    "LexToken(ID,'j',2,18)\n",
    "LexToken(EQUALS,'=',2,20)\n",
    "LexToken(INT_CONST,'3',2,22)\n",
    "LexToken(SEMI,';',2,23)\n",
    "LexToken(INT,'int',3,25)\n",
    "LexToken(ID,'main',3,29)\n",
    "LexToken(LPAREN,'(',3,34)\n",
    "LexToken(RPAREN,')',3,35)\n",
    "LexToken(LBRACE,'{',3,37)\n",
    "LexToken(INT,'int',4,41)\n",
    "LexToken(ID,'i',4,45)\n",
    "LexToken(EQUALS,'=',4,47)\n",
    "LexToken(ID,'j',4,49)\n",
    "LexToken(SEMI,';',4,50)\n",
    "LexToken(INT,'int',5,54)\n",
    "LexToken(ID,'k',5,58)\n",
    "LexToken(EQUALS,'=',5,60)\n",
    "LexToken(INT_CONST,'3',5,62)\n",
    "LexToken(SEMI,';',5,63)\n",
    "LexToken(INT,'int',6,67)\n",
    "LexToken(ID,'p',6,71)\n",
    "LexToken(EQUALS,'=',6,73)\n",
    "LexToken(INT_CONST,'2',6,75)\n",
    "LexToken(TIMES,'*',6,77)\n",
    "LexToken(ID,'j',6,79)\n",
    "LexToken(SEMI,';',6,80)\n",
    "LexToken(ASSERT,'assert',7,84)\n",
    "LexToken(ID,'p',7,91)\n",
    "LexToken(EQ,'==',7,93)\n",
    "LexToken(INT_CONST,'2',7,96)\n",
    "LexToken(TIMES,'*',7,98)\n",
    "LexToken(ID,'i',7,100)\n",
    "LexToken(SEMI,';',7,101)\n",
    "LexToken(RBRACE,'}',8,103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully study the output of the lexer and make sure that it makes sense. Once you are reasonably happy with the output, try running some of the more [tricky tests](./lex_unit_tests.ipynb) designed to stress test various corner cases. How would you go about turning these tests into proper unit tests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you write the basic shell of a parser for the uC language. A formal BNF of the language is [here](./uC_Grammar.ipynb). Your task is to write parsing rules and build the AST for this grammar using PLY. Parsers are defined using PLY’s yacc module (see [PLY-Yacc](http://www.dabeaz.com/ply/ply.html#ply_nn22) documentation).\n",
    "\n",
    "Your task is translate the BNF into a collection of parser functions. For example, a rule such as :\n",
    "```\n",
    "  <program> ::= {<global_declaration>}+\n",
    "```  \n",
    "Gets turned into a Python function of the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    ...\n",
    "    def p_program(self, p):\n",
    "        \"\"\" program  : global_declaration_list\n",
    "        \"\"\"\n",
    "        p[0] = Program(p[1])\n",
    "\n",
    "    def p_global_declaration_list(self, p):\n",
    "        \"\"\" global_declaration_list : global_declaration\n",
    "                                    | global_declaration_list global_declaration\n",
    "        \"\"\"\n",
    "        p[0] = [p[1]] if len(p) == 2 else p[1] + [p[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the body of each rule, create an appropriate AST node and assign it to p[0] as shown above.\n",
    "\n",
    "For the purposes of lineno number tracking, you should assign a line number to each AST node as appropriate. To do this, I suggest pulling the line number off of any nearby terminal symbol. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def p_identifier(self, p):\n",
    "        \"\"\" identifier : ID \"\"\"\n",
    "        p[0] = ID(p[1], lineno=p.lineno(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Syntax Tree objects\n",
    "This section defines classes for different kinds of nodes of an Abstract Syntax Tree. During parsing, you will create these nodes and connect them together. In general, you will have a different AST node for each kind of grammar rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class example for the AST nodes.  Each node is expected to\n",
    "    define the _fields attribute which lists the names of stored\n",
    "    attributes.   The __init__() method below takes positional\n",
    "    arguments and assigns them to the appropriate fields.  Any\n",
    "    additional arguments specified as keywords are also assigned.\n",
    "    \"\"\"\n",
    "    _fields = []\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        assert len(args) == len(self._fields)\n",
    "        for name,value in zip(self._fields,args):\n",
    "            setattr(self,name,value)\n",
    "        # Assign additional keyword arguments if supplied\n",
    "        for name,value in kwargs.items():\n",
    "            setattr(self,name,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of specific AST nodes, you need to add the appropriate\n",
    "```\n",
    "_fields = []\n",
    "```\n",
    "specification that indicates what fields are to be stored: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the top of the AST, representing a uC program (a\n",
    "# translation unit in K&R jargon). It contains a list of\n",
    "# global-declaration's, which is either declarations (Decl),\n",
    "# or function definitions (FuncDef).\n",
    "class Program(Node):\n",
    "    _fields = ['decls']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as another example, for a binary operator, you might store the operator, the left expression, and the right expression like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryOp(Node):\n",
    "    _fields = ['lvalue', 'op', 'rvalue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: You should start simple and incrementally work your way up to building the complete grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST node classes\n",
    "\n",
    "The list below defines the AST node classes that must be used in uCParser:\n",
    "\n",
    "ArrayDecl, ArrayRef, Assert, Assignment, BinaryOp, Break, Cast, Compound, Constant, Decl, DeclList, EmptyStatement, ExprList, For, FuncCall, FuncDecl, FuncDef, GlobalDecl, ID, If, InitList, ParamList, Print, Program, PtrDecl, Read, Return, Type, VarDecl, UnaryOp, While."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visiting the AST\n",
    "The following classes for visiting the AST are taken from Python’s ast module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeVisitor(object):\n",
    "    \"\"\" A base NodeVisitor class for visiting uc_ast nodes.\n",
    "        Subclass it and define your own visit_XXX methods, where\n",
    "        XXX is the class name you want to visit with these\n",
    "        methods.\n",
    "\n",
    "        For example:\n",
    "\n",
    "        class ConstantVisitor(NodeVisitor):\n",
    "            def __init__(self):\n",
    "                self.values = []\n",
    "\n",
    "            def visit_Constant(self, node):\n",
    "                self.values.append(node.value)\n",
    "\n",
    "        Creates a list of values of all the constant nodes\n",
    "        encountered below the given node. To use it:\n",
    "\n",
    "        cv = ConstantVisitor()\n",
    "        cv.visit(node)\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        *   generic_visit() will be called for AST nodes for which\n",
    "            no visit_XXX method was defined.\n",
    "        *   The children of nodes for which a visit_XXX was\n",
    "            defined will not be visited - if you need this, call\n",
    "            generic_visit() on the node.\n",
    "            You can use:\n",
    "                NodeVisitor.generic_visit(self, node)\n",
    "        *   Modeled after Python's own AST visiting facilities\n",
    "            (the ast module of Python 3.0)\n",
    "    \"\"\"\n",
    "\n",
    "    _method_cache = None\n",
    "\n",
    "    def visit(self, node):\n",
    "        \"\"\" Visit a node.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._method_cache is None:\n",
    "            self._method_cache = {}\n",
    "\n",
    "        visitor = self._method_cache.get(node.__class__.__name__, None)\n",
    "        if visitor is None:\n",
    "            method = 'visit_' + node.__class__.__name__\n",
    "            visitor = getattr(self, method, self.generic_visit)\n",
    "            self._method_cache[node.__class__.__name__] = visitor\n",
    "\n",
    "        return visitor(node)\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        \"\"\" Called if no explicit visitor function exists for a\n",
    "            node. Implements preorder visiting of the node.\n",
    "        \"\"\"\n",
    "        for c in node:\n",
    "            self.visit(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the AST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the previous uC program example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* comment */\n",
    "int j = 3;\n",
    "int main () {\n",
    "  int i = j;\n",
    "  int k = 3;\n",
    "  int p = 2 * j;\n",
    "  assert p == 2 * i;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible dump of the AST looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Program: \n",
    "    GlobalDecl: \n",
    "        Decl: ID(name='j'  )\n",
    "            VarDecl: ID(name='j'  )\n",
    "                Type: ['int']   @ 2:1\n",
    "            Constant: int, 3   @ 2:9\n",
    "    FuncDef: \n",
    "        Type: ['int']   @ 3:1\n",
    "        Decl: ID(name='main'  )\n",
    "            FuncDecl: \n",
    "                VarDecl: ID(name='main'  )\n",
    "                    Type: ['int']   @ 3:1\n",
    "        Compound:    @ 3:1\n",
    "            Decl: ID(name='i'  )\n",
    "                VarDecl: ID(name='i'  )\n",
    "                    Type: ['int']   @ 4:3\n",
    "                ID: j   @ 4:11\n",
    "            Decl: ID(name='k'  )\n",
    "                VarDecl: ID(name='k'  )\n",
    "                    Type: ['int']   @ 5:3\n",
    "                Constant: int, 3   @ 5:11\n",
    "            Decl: ID(name='p'  )\n",
    "                VarDecl: ID(name='p'  )\n",
    "                    Type: ['int']   @ 6:3\n",
    "                BinaryOp: *   @ 6:11\n",
    "                    Constant: int, 2   @ 6:11\n",
    "                    ID: j   @ 6:15\n",
    "            Assert:    @ 7:3\n",
    "                BinaryOp: ==   @ 7:10\n",
    "                    ID: p   @ 7:10\n",
    "                    BinaryOp: *   @ 7:15\n",
    "                        Constant: int, 2   @ 7:15\n",
    "                        ID: i   @ 7:19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the methods to generate a textual representation of the nodes and print all its attributes is showing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _repr(obj):\n",
    "    \"\"\"\n",
    "    Get the representation of an object, with dedicated pprint-like format for lists.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        return '[' + (',\\n '.join((_repr(e).replace('\\n', '\\n ') for e in obj))) + '\\n]'\n",
    "    else:\n",
    "        return repr(obj) \n",
    "    \n",
    "class Node(object):\n",
    "    \"\"\" Abstract base class for AST nodes.\n",
    "    \"\"\"\n",
    "    def __repr__(self):\n",
    "        \"\"\" Generates a python representation of the current node\n",
    "        \"\"\"\n",
    "        result = self.__class__.__name__ + '('\n",
    "        indent = ''\n",
    "        separator = ''\n",
    "        for name in self.__slots__[:-2]:\n",
    "            result += separator\n",
    "            result += indent\n",
    "            result += name + '=' + (_repr(getattr(self, name)).replace('\\n', '\\n  ' + (' ' * (len(name) + len(self.__class__.__name__)))))\n",
    "            separator = ','\n",
    "            indent = ' ' * len(self.__class__.__name__)\n",
    "        result += indent + ')'\n",
    "        return result\n",
    "\n",
    "    def children(self):\n",
    "        \"\"\" A sequence of all children that are Nodes\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def show(self, buf=sys.stdout, offset=0, attrnames=False, nodenames=False, showcoord=False, _my_node_name=None):\n",
    "        \"\"\" Pretty print the Node and all its attributes and children (recursively) to a buffer.\n",
    "            buf:\n",
    "                Open IO buffer into which the Node is printed.\n",
    "            offset:\n",
    "                Initial offset (amount of leading spaces)\n",
    "            attrnames:\n",
    "                True if you want to see the attribute names in name=value pairs. False to only see the values.\n",
    "            nodenames:\n",
    "                True if you want to see the actual node names within their parents.\n",
    "            showcoord:\n",
    "                Do you want the coordinates of each Node to be displayed.\n",
    "        \"\"\"\n",
    "        lead = ' ' * offset\n",
    "        if nodenames and _my_node_name is not None:\n",
    "            buf.write(lead + self.__class__.__name__+ ' <' + _my_node_name + '>: ')\n",
    "        else:\n",
    "            buf.write(lead + self.__class__.__name__+ ': ')\n",
    "\n",
    "        if self.attr_names:\n",
    "            if attrnames:\n",
    "                nvlist = [(n, getattr(self, n)) for n in self.attr_names if getattr(self, n) is not None]\n",
    "                attrstr = ', '.join('%s=%s' % nv for nv in nvlist)\n",
    "            else:\n",
    "                vlist = [getattr(self, n) for n in self.attr_names]\n",
    "                attrstr = ', '.join('%s' % v for v in vlist)\n",
    "            buf.write(attrstr)\n",
    "\n",
    "        if showcoord:\n",
    "            if self.coord:\n",
    "                buf.write('%s' % self.coord)\n",
    "        buf.write('\\n')\n",
    "\n",
    "        for (child_name, child) in self.children():\n",
    "            child.show(buf, offset + 4, attrnames, nodenames, showcoord, child_name)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
