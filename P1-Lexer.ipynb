{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Project: Lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first project requires you to implement a scanner for the uC language,\n",
    "specified by [uC BNF Grammar](./doc/uC_Grammar.ipynb) notebook. Study the specification\n",
    "of uC grammar carefully. To complete this first project, you will use the\n",
    "[PLY](http://www.dabeaz.com/ply/), a Python version of the\n",
    "[lex/yacc](http://dinosaur.compilertools.net/) toolset with same functionality\n",
    "but with a friendlier interface. Please read the complete contents of this section\n",
    "and carefully complete the steps indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are concise ways of describing a set of strings that meet a given\n",
    "pattern. For example, we can specify the regular expression:\n",
    "```python\n",
    "r'[a-zA-Z_][0-9a-zA-Z_]*'\n",
    "``` \n",
    "to describe valid identifiers in the uC language. Regular expressions are a mini-language\n",
    "that lets you specify the rules for constructing a string set. This specification\n",
    "mini-language is very similar between the different programming languages that contain\n",
    "the concept of regular expressions (also called RE or REGEX). Thus, learning to write\n",
    "regular expressions in Python will also be useful for describing REs in other programming\n",
    "languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a set of regular expressions that will be used by the lexical\n",
    "parser to recognize the following patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# valid uC identifiers\n",
    "identifier ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# integer constants\n",
    "int_const = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comments in C-Style /* ... */\n",
    "ccomment = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Unterminated C-style comment\n",
    "uccomment = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# C++-style comment (//...)\n",
    "cppcomment = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# string_literal\n",
    "string_literal = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# unmatched_quote\n",
    "unquote = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "import re\n",
    "b = re.match(ccomment, \"/***/\")\n",
    "if b:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Lexer\n",
    "The process of “lexing” is that of taking input text and breaking it down into a stream\n",
    "of tokens. Each token is like a valid word from the dictionary. Essentially, the role of\n",
    "the lexer is to simply make sure that the input text consists of valid symbols and tokens\n",
    "prior to any further processing related to parsing.\n",
    "\n",
    "Each token is defined by a regular expression. Thus, your task here is to define a set of\n",
    "regular expressions for the uC language. The actual job of lexing will be handled by PLY.\n",
    "For a better understanding study the [Lex](http://www.dabeaz.com/ply/ply.html#ply_nn3)\n",
    "chapter in the PLY documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification\n",
    "Your lexer must recognize the symbols and tokens of uC Grammar. For instance, in the\n",
    "example below, the name on the left is the token name, and the value on the right is\n",
    "the matching text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserved Keywords:\n",
    "```\n",
    "    FOR   : 'for'\n",
    "    IF    : 'if'\n",
    "    PRINT : 'print'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers:\n",
    "```\n",
    "    ID    : any text starting with a letter or '_', followed by any number of letters,\n",
    "            digits, or underscores, that is not a reserved word.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Operators and Delimiters:\n",
    "```\n",
    "    PLUS    : '+'\n",
    "    MINUS   : '-'\n",
    "    TIMES   : '*'\n",
    "    DIVIDE  : '/'\n",
    "    ASSIGN  : '='\n",
    "    SEMI    : ';'\n",
    "    LPAREN  : '('\n",
    "    RPAREN  : ')'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literals:\n",
    "```\n",
    "    INT_CONST : 123\n",
    "    STRING_LITERAL : \"Hello World\\n\"\n",
    "```\n",
    "\n",
    "For `INT_CONST`, you should only consider decimal numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:  To be ignored by your lexer\n",
    "```\n",
    "     //             Skips the rest of the line\n",
    "     /* ... */      Skips a block (no nesting allowed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors: Your lexer must report the following error messages:\n",
    "```\n",
    "     lineno: Unterminated string\n",
    "     lineno: Unterminated comment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lex Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ply.lex as lex\n",
    "\n",
    "\n",
    "class UCLexer:\n",
    "    \"\"\"A lexer for the uC language. After building it, set the\n",
    "    input text with input(), and call token() to get new\n",
    "    tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, error_func):\n",
    "        \"\"\"Create a new Lexer.\n",
    "        An error function. Will be called with an error\n",
    "        message, line and column as arguments, in case of\n",
    "        an error during lexing.\n",
    "        \"\"\"\n",
    "        self.error_func = error_func\n",
    "        self.filename = \"\"\n",
    "\n",
    "        # Keeps track of the last token returned from self.token()\n",
    "        self.last_token = None\n",
    "\n",
    "    def build(self, **kwargs):\n",
    "        \"\"\"Builds the lexer from the specification. Must be\n",
    "        called after the lexer object is created.\n",
    "\n",
    "        This method exists separately, because the PLY\n",
    "        manual warns against calling lex.lex inside __init__\n",
    "        \"\"\"\n",
    "        self.lexer = lex.lex(object=self, **kwargs)\n",
    "\n",
    "    def reset_lineno(self):\n",
    "        \"\"\"Resets the internal line number counter of the lexer.\"\"\"\n",
    "        self.lexer.lineno = 1\n",
    "\n",
    "    def input(self, text):\n",
    "        self.lexer.input(text)\n",
    "\n",
    "    def token(self):\n",
    "        self.last_token = self.lexer.token()\n",
    "        return self.last_token\n",
    "\n",
    "    def find_tok_column(self, token):\n",
    "        \"\"\"Find the column of the token in its line.\"\"\"\n",
    "        last_cr = self.lexer.lexdata.rfind(\"\\n\", 0, token.lexpos)\n",
    "        return token.lexpos - last_cr\n",
    "\n",
    "    # Internal auxiliary methods\n",
    "    def _error(self, msg, token):\n",
    "        location = self._make_tok_location(token)\n",
    "        self.error_func(msg, location[0], location[1])\n",
    "        self.lexer.skip(1)\n",
    "\n",
    "    def _make_tok_location(self, token):\n",
    "        return token.lineno, self.find_tok_column(token)\n",
    "\n",
    "    # Reserved keywords\n",
    "    keywords = (\n",
    "        \"ASSERT\",\n",
    "        \"BREAK\",\n",
    "        \"CHAR\",\n",
    "        \"ELSE\",\n",
    "        \"FOR\",\n",
    "        \"IF\",\n",
    "        \"INT\",\n",
    "        \"PRINT\",\n",
    "        \"READ\",\n",
    "        \"RETURN\",\n",
    "        \"VOID\",\n",
    "        \"WHILE\",\n",
    "    )\n",
    "\n",
    "    keyword_map = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_map[keyword.lower()] = keyword\n",
    "\n",
    "    #\n",
    "    # All the tokens recognized by the lexer\n",
    "    #\n",
    "    tokens = keywords + (\n",
    "        # Identifiers\n",
    "        \"ID\",\n",
    "        # constants\n",
    "        \"INT_CONST\",\n",
    "        \"CHAR_CONST\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Rules\n",
    "    #\n",
    "    t_ignore = \" \\t\"\n",
    "\n",
    "    # Newlines\n",
    "    def t_NEWLINE(self, t):\n",
    "        # include a regex here for newline\n",
    "        t.lexer.lineno += t.value.count(\"\\n\")\n",
    "\n",
    "    def t_ID(self, t):\n",
    "        # include a regex here for ID\n",
    "        t.type = self.keyword_map.get(t.value, \"ID\")\n",
    "        return t\n",
    "\n",
    "    def t_comment(self, t):\n",
    "        # include a regex here for comment\n",
    "        t.lexer.lineno += t.value.count(\"\\n\")\n",
    "\n",
    "    def t_error(self, t):\n",
    "        msg = \"Illegal character %s\" % repr(t.value[0])\n",
    "        self._error(msg, t)\n",
    "\n",
    "    # Scanner (used only for test)\n",
    "    def scan(self, data):\n",
    "        self.lexer.input(data)\n",
    "        output = \"\"\n",
    "        while True:\n",
    "            tok = self.lexer.token()\n",
    "            if not tok:\n",
    "                break\n",
    "            print(tok)\n",
    "            output += str(tok) + \"\\n\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "For initial development, try running the lexer on a sample input file such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "```cpp\n",
    "/* comment */\n",
    "int j = 3;\n",
    "int main () {\n",
    "  int i = j;\n",
    "  int k = 3;\n",
    "  int p = 2 * j;\n",
    "  assert p == 2 * i;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result will look similar to the text shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "```\n",
    "LexToken(INT,'int',2,14)\n",
    "LexToken(ID,'j',2,18)\n",
    "LexToken(EQUALS,'=',2,20)\n",
    "LexToken(INT_CONST,'3',2,22)\n",
    "LexToken(SEMI,';',2,23)\n",
    "LexToken(INT,'int',3,25)\n",
    "LexToken(ID,'main',3,29)\n",
    "LexToken(LPAREN,'(',3,34)\n",
    "LexToken(RPAREN,')',3,35)\n",
    "LexToken(LBRACE,'{',3,37)\n",
    "LexToken(INT,'int',4,41)\n",
    "LexToken(ID,'i',4,45)\n",
    "LexToken(EQUALS,'=',4,47)\n",
    "LexToken(ID,'j',4,49)\n",
    "LexToken(SEMI,';',4,50)\n",
    "LexToken(INT,'int',5,54)\n",
    "LexToken(ID,'k',5,58)\n",
    "LexToken(EQUALS,'=',5,60)\n",
    "LexToken(INT_CONST,'3',5,62)\n",
    "LexToken(SEMI,';',5,63)\n",
    "LexToken(INT,'int',6,67)\n",
    "LexToken(ID,'p',6,71)\n",
    "LexToken(EQUALS,'=',6,73)\n",
    "LexToken(INT_CONST,'2',6,75)\n",
    "LexToken(TIMES,'*',6,77)\n",
    "LexToken(ID,'j',6,79)\n",
    "LexToken(SEMI,';',6,80)\n",
    "LexToken(ASSERT,'assert',7,84)\n",
    "LexToken(ID,'p',7,91)\n",
    "LexToken(EQ,'==',7,93)\n",
    "LexToken(INT_CONST,'2',7,96)\n",
    "LexToken(TIMES,'*',7,98)\n",
    "LexToken(ID,'i',7,100)\n",
    "LexToken(SEMI,';',7,101)\n",
    "LexToken(RBRACE,'}',8,103)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully study the output of the lexer and make sure that it makes sense. Once you are\n",
    "reasonably happy with the output, try running some of the more tricky tests designed to stress test various corner cases. The repository provided as a base to implement the project contains a large set of tests to verify your code: check them to see more examples.\n",
    "\n",
    "Here is another example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "```cpp\n",
    "int v[5] = { 1, 3, 5, 7, 9};\n",
    "assert v[3] == 7;\n",
    "```\n",
    "\n",
    "```\n",
    "LexToken(INT,'int',1,0)\n",
    "LexToken(ID,'v',1,4)\n",
    "LexToken(LBRACKET,'[',1,5)\n",
    "LexToken(INT_CONST,'5',1,6)\n",
    "LexToken(RBRACKET,']',1,7)\n",
    "LexToken(EQUALS,'=',1,9)\n",
    "LexToken(LBRACE,'{',1,11)\n",
    "LexToken(INT_CONST,'1',1,13)\n",
    "LexToken(COMMA,',',1,14)\n",
    "LexToken(INT_CONST,'3',1,16)\n",
    "LexToken(COMMA,',',1,17)\n",
    "LexToken(INT_CONST,'5',1,19)\n",
    "LexToken(COMMA,',',1,20)\n",
    "LexToken(INT_CONST,'7',1,22)\n",
    "LexToken(COMMA,',',1,23)\n",
    "LexToken(INT_CONST,'9',1,25)\n",
    "LexToken(RBRACE,'}',1,26)\n",
    "LexToken(SEMI,';',1,27)\n",
    "LexToken(ASSERT,'assert',2,29)\n",
    "LexToken(ID,'v',2,36)\n",
    "LexToken(LBRACKET,'[',2,37)\n",
    "LexToken(INT_CONST,'3',2,38)\n",
    "LexToken(RBRACKET,']',2,39)\n",
    "LexToken(EQ,'==',2,41)\n",
    "LexToken(INT_CONST,'7',2,44)\n",
    "LexToken(SEMI,';',2,45)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
